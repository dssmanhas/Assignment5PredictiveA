# -*- coding: utf-8 -*-
"""Assignment5-Predictive.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1miIMsc-uIMs7QiITAqV-nhA3cVrfdlWu
"""

# Clustering Analysis using PyCaret in Google Colab

# Install PyCaret if not already installed

# Import necessary libraries
import pandas as pd
from pycaret.clustering import *
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# Load dataset from UCI repository (Example: Iris dataset)
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
columns = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']
df = pd.read_csv(url, names=columns)
df = df.drop(columns=['class'])  # Removing the target variable as clustering is unsupervised

# Preprocessing techniques
def preprocess_data(df, method):
    if method == 'standard':
        scaler = StandardScaler()
    elif method == 'minmax':
        scaler = MinMaxScaler()
    else:
        return df  # No scaling
    return pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

# Define clustering configurations
preprocessing_methods = ['none', 'standard', 'minmax']
cluster_algorithms = ['kmeans', 'ap', 'meanshift', 'hclust']  # Affinity Propagation, Mean Shift, Hierarchical Clustering
cluster_numbers = [3, 4, 5]  # Different cluster numbers to compare

# Perform clustering with different settings
results = []
for pre_method in preprocessing_methods:
    df_preprocessed = preprocess_data(df, pre_method)

    for algo in cluster_algorithms:
        for clusters in cluster_numbers:
            print(f"Running {algo} with {clusters} clusters and {pre_method} scaling")
            setup_data = setup(df_preprocessed)
            model = create_model(algo, num_clusters=clusters) if algo == 'kmeans' else create_model(algo)
            eval_metrics = pull()  # Retrieve evaluation metrics
            eval_metrics['algorithm'] = algo
            eval_metrics['clusters'] = clusters
            eval_metrics['preprocessing'] = pre_method
            results.append(eval_metrics)

# Convert results to DataFrame and display summary
results_df = pd.concat(results, ignore_index=True)
print(results_df)

# Save results for further analysis
results_df.to_csv('clustering_results.csv', index=False)

df.shape

!pip install pycaret

!pip install pycaret